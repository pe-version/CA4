# CA4 Multi-Hybrid Cloud - Design Decisions Tracker

**Student**: Philip Eykamp
**Course**: CS 5287
**Assignment**: CA4 - Multi-Hybrid Cloud Deployment
**Date**: November 2025

---

## Executive Summary

This document tracks all critical design decisions for CA4, building on the existing CA3 AWS Kubernetes infrastructure. The goal is to extend to a multi-cloud deployment demonstrating cross-cloud connectivity, resilience, and operational excellence.

---

## CA3 Baseline (What We're Building On)

### Current Infrastructure
- **Platform**: K3s on AWS (us-east-2)
- **Nodes**: 3 nodes (2x t3.medium + 1x t3.small) = 10GB RAM total
- **Cost**: ~$75/month (~$2.50/day)
- **Status**: âœ… All services operational

### Current Components (AWS)
```
AWS K3s Cluster:
â”œâ”€â”€ Data Layer (Worker-1, t3.medium)
â”‚   â”œâ”€â”€ Kafka StatefulSet (1 replica, 1.5GB RAM, TLS ready)
â”‚   â”œâ”€â”€ Zookeeper StatefulSet (1 replica, 512MB RAM)
â”‚   â””â”€â”€ MongoDB StatefulSet (1 replica, 1GB RAM, TLS enabled)
â”‚
â”œâ”€â”€ Application Layer (Worker-2, t3.small)
â”‚   â”œâ”€â”€ Producer Deployment (1-3 replicas via HPA)
â”‚   â””â”€â”€ Processor Deployment (1-3 replicas via HPA)
â”‚
â””â”€â”€ Observability (Master, t3.medium)
    â”œâ”€â”€ Prometheus + Grafana
    â”œâ”€â”€ Loki + Promtail
    â””â”€â”€ External Secrets Operator
```

### Existing Features
- âœ… Full observability (Prometheus, Grafana, Loki)
- âœ… Autoscaling (HPA for Producer/Processor)
- âœ… TLS encryption (MongoDB + Kafka dual listeners)
- âœ… Network policies (9 policies deployed)
- âœ… Secrets management (External Secrets Operator + AWS Secrets Manager)

---

## DECISION FRAMEWORK

For each decision below:
- **STATUS**: ğŸ”´ Pending | ğŸŸ¡ Under Review | ğŸŸ¢ Decided
- **DECISION DATE**: When finalized
- **RATIONALE**: Why this choice
- **IMPACT**: Cost, complexity, timeline
- **ALTERNATIVES CONSIDERED**: What else was evaluated

---

## ğŸ”´ DECISION 1: Second Cloud Provider Selection

**STATUS**: ğŸ”´ Pending
**PRIORITY**: Critical (blocks all other decisions)
**DECISION NEEDED BY**: Before infrastructure provisioning

### Options Analysis

#### Option A: Google Cloud Platform (GCP) â­ RECOMMENDED FOR LEARNING
**Free Tier**: $300 credit (90 days)
**Cost After Credits**: ~$60/month for GKE cluster

**Pros**:
- Excellent Kubernetes (GKE) - native platform, similar to AWS EKS
- Strong VPN support (Cloud VPN with HA options)
- $300 free credit = 3-5 months free for small cluster
- Industry leader - best for resume/portfolio
- Similar AWS experience (IaC friendly)
- Strong network performance
- Good Terraform provider

**Cons**:
- Credit card required
- Credits expire after 90 days (must monitor)
- More expensive after credits (~$60/month)
- Learning curve for GCP-specific services

**Total CA4 Cost** (with credits):
- AWS: $75/month (existing)
- GCP: $0/month for 3 months, then $60/month
- **Total**: $75/month (during free tier), $135/month after

---

#### Option B: Azure
**Free Tier**: $200 credit (30 days) + 12 months free services
**Cost After Credits**: ~$55/month for AKS cluster

**Pros**:
- Mature AKS (Azure Kubernetes Service)
- $200 credit + 12 months free tier for some services
- Good enterprise integration (if targeting corporate jobs)
- Strong Terraform support

**Cons**:
- Credit card required
- More complex networking than GCP/AWS
- Only 30 days of credits (vs GCP's 90)
- Steeper learning curve (different terminology)
- VPN setup more complex

**Total CA4 Cost**:
- AWS: $75/month
- Azure: $0 first month, then ~$55/month
- **Total**: $75/month (first month), $130/month after

---

#### Option C: DigitalOcean ğŸ’° RECOMMENDED FOR COST
**Free Tier**: $200 credit (60 days) with promo codes
**Cost After Credits**: ~$40/month for DOKS cluster

**Pros**:
- Simplest pricing model (very transparent)
- Managed Kubernetes (DOKS) is straightforward
- Very good documentation (beginner-friendly)
- No credit card needed for trial (with promo codes)
- Lowest cost after credits (~$40/month vs $55-60)
- Simple VPN setup (DigitalOcean VPN or WireGuard)
- Fast provisioning (minutes vs AWS/GCP)

**Cons**:
- Smaller ecosystem than AWS/GCP/Azure
- Less enterprise features (may not matter for assignment)
- Not as "impressive" on resume as GCP/AWS

**Total CA4 Cost** (with credits):
- AWS: $75/month
- DO: $0 for 2 months, then $40/month
- **Total**: $75/month (during free tier), $115/month after

---

#### Option D: Linode (Akamai)
**Free Tier**: $100 credit (60 days)
**Cost After Credits**: ~$35-50/month

**Pros**:
- Simple pricing
- Good documentation
- Managed Kubernetes (LKE)

**Cons**:
- Smaller ecosystem than others
- Less industry recognition
- Lower credit amount ($100 vs $200-300)

**Total CA4 Cost**:
- AWS: $75/month
- Linode: $0 for 2 months, then $35-50/month
- **Total**: $75/month (during free tier), $110-125/month after

---

### Decision Matrix

| Factor | GCP | Azure | DigitalOcean | Linode |
|--------|-----|-------|--------------|--------|
| **Free Credits** | $300 (90d) | $200 (30d) | $200 (60d) | $100 (60d) |
| **Free Duration** | 3 months | 1 month | 2 months | 2 months |
| **Cost After Credits** | $60/mo | $55/mo | $40/mo | $35-50/mo |
| **Learning Value** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­ |
| **Resume Value** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­ |
| **Ease of Setup** | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| **Documentation** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| **Kubernetes Quality** | â­â­â­â­â­ (GKE) | â­â­â­â­ (AKS) | â­â­â­â­ (DOKS) | â­â­â­â­ (LKE) |

---

### My Recommendation

**FOR LEARNING/CAREER**: GCP (Option A)
- Best multi-cloud experience (AWS + GCP = 2 major cloud providers)
- Longest free tier (90 days)
- Industry standard skills
- Best for portfolio/resume

**FOR COST**: DigitalOcean (Option C)
- Lowest long-term cost ($115/mo vs $135/mo for GCP)
- Simplest setup = less debugging time
- Still demonstrates multi-cloud patterns
- Good enough for assignment requirements

**DECISION**: â“ **PENDING USER INPUT**

---

## ğŸ”´ DECISION 2: Topology Pattern

**STATUS**: ğŸ”´ Pending
**PRIORITY**: Critical (affects component distribution)
**DECISION NEEDED BY**: Before deployment planning

### Option A: Multi-Cloud Split (Data vs Compute) â­ RECOMMENDED

```
AWS (us-east-2):                      GCP/DO (Cloud2):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DATA TIER               â”‚          â”‚ COMPUTE TIER            â”‚
â”‚                         â”‚          â”‚                         â”‚
â”‚ â€¢ Kafka StatefulSet     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â€¢ Producer Deployment   â”‚
â”‚   (3 replicas)          â”‚  VPN    â”‚   (1-3 replicas)        â”‚
â”‚                         â”‚  Tunnel â”‚                         â”‚
â”‚ â€¢ Zookeeper StatefulSet â”‚          â”‚ â€¢ Processor Deployment  â”‚
â”‚   (1 replica)           â”‚          â”‚   (1-3 replicas)        â”‚
â”‚                         â”‚          â”‚                         â”‚
â”‚ â€¢ MongoDB StatefulSet   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                         â”‚
â”‚   (1 replica)           â”‚          â”‚                         â”‚
â”‚                         â”‚          â”‚ OBSERVABILITY           â”‚
â”‚ OBSERVABILITY HUB       â”‚          â”‚ â€¢ Promtail (logs)       â”‚
â”‚ â€¢ Prometheus            â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â€¢ Node Exporter         â”‚
â”‚ â€¢ Grafana               â”‚          â”‚                         â”‚
â”‚ â€¢ Loki                  â”‚          â”‚                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        Existing CA3                       New in CA4
```

**Data Flow**:
1. Producer (Cloud2) â†’ Kafka (AWS) via VPN
2. Processor (Cloud2) â†’ Kafka (AWS) â†’ MongoDB (AWS) via VPN
3. Promtail (Cloud2) â†’ Loki (AWS) via VPN
4. Prometheus (AWS) scrapes both clouds

**Pros**:
- âœ… Clean separation of concerns (data persistence vs compute)
- âœ… Demonstrates cross-cloud connectivity clearly
- âœ… Reuses existing CA3 AWS infrastructure (Kafka, MongoDB, observability)
- âœ… Clear failure scenario (VPN down â†’ processors can't reach Kafka)
- âœ… Realistic pattern (centralized data, distributed compute)
- âœ… Easy to understand and explain

**Cons**:
- âŒ Higher latency (Kafka â†” Processor cross-cloud)
- âŒ More network bandwidth usage
- âŒ VPN dependency (single point of failure)

**Failure Scenario**:
```bash
# Simulate VPN tunnel failure
kubectl delete -n kube-system deployment/wireguard

# Observe:
# - Processors in Cloud2 can't reach Kafka in AWS
# - Producer messages queue up or fail
# - Grafana dashboard shows connection errors

# Recovery:
kubectl apply -f wireguard.yaml
# - VPN tunnel re-establishes
# - Processors reconnect to Kafka
# - Messages resume processing
```

---

### Option B: Edge â†’ Cloud

```
Local (Your Laptop):                 AWS (us-east-2):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EDGE TIER               â”‚          â”‚ CLOUD TIER              â”‚
â”‚                         â”‚          â”‚                         â”‚
â”‚ â€¢ Producer (Docker/K3s) â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ â€¢ Kafka StatefulSet     â”‚
â”‚                         â”‚  VPN/SSH â”‚ â€¢ MongoDB StatefulSet   â”‚
â”‚                         â”‚  Tunnel  â”‚ â€¢ Processor Deployment  â”‚
â”‚                         â”‚          â”‚                         â”‚
â”‚                         â”‚          â”‚ â€¢ Prometheus            â”‚
â”‚                         â”‚          â”‚ â€¢ Grafana               â”‚
â”‚                         â”‚          â”‚ â€¢ Loki                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros**:
- âœ… Lowest cost (only one cloud + local)
- âœ… Demonstrates edge computing pattern (IoT scenario)
- âœ… Easy to test locally
- âœ… Can simulate edge device scenarios

**Cons**:
- âŒ Requires laptop always on during testing
- âŒ NAT traversal complexity (firewall issues)
- âŒ Less impressive than multi-cloud
- âŒ Not truly "multi-cloud" (may not meet assignment requirements)
- âŒ Network reliability dependent on home internet

---

### Option C: Active-Active Multi-Cloud

```
AWS (us-east-2):                      GCP (Cloud2):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CLUSTER 1               â”‚          â”‚ CLUSTER 2               â”‚
â”‚                         â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                         â”‚
â”‚ â€¢ Kafka-1               â”‚ Mirroringâ”‚ â€¢ Kafka-2               â”‚
â”‚ â€¢ MongoDB-1 (Primary)   â”‚ Replicat â”‚ â€¢ MongoDB-2 (Secondary) â”‚
â”‚ â€¢ Producer-1            â”‚          â”‚ â€¢ Producer-2            â”‚
â”‚ â€¢ Processor-1           â”‚          â”‚ â€¢ Processor-2           â”‚
â”‚                         â”‚          â”‚                         â”‚
â”‚ â€¢ Prometheus            â”‚          â”‚ â€¢ Prometheus            â”‚
â”‚ â€¢ Grafana               â”‚          â”‚ â€¢ Grafana               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros**:
- âœ… Most resilient (true HA)
- âœ… True multi-cloud HA
- âœ… Geographic distribution
- âœ… Impressive technically

**Cons**:
- âŒ Most complex setup
- âŒ Kafka MirrorMaker is challenging
- âŒ MongoDB replica set across clouds (latency issues)
- âŒ Double the cost (running full stack in both clouds)
- âŒ Overkill for assignment (may not be graded higher)

---

### Decision Matrix

| Factor | Multi-Cloud Split | Edgeâ†’Cloud | Active-Active |
|--------|------------------|------------|---------------|
| **Complexity** | Medium | Low | Very High |
| **Cost** | Medium | Low | High (2x) |
| **Learning Value** | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ |
| **Assignment Fit** | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ |
| **Clear Demo** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ |
| **Reliability** | â­â­â­ | â­â­ | â­â­â­â­â­ |
| **Setup Time** | 4-6 hours | 2-3 hours | 12-16 hours |

---

### My Recommendation

**RECOMMENDED**: Option A - Multi-Cloud Split

**Rationale**:
- Best balance of complexity vs demonstration value
- Clearly shows multi-cloud connectivity (assignment goal)
- Reuses CA3 infrastructure (efficient)
- Easy to explain and demonstrate
- Realistic failure scenario
- Achievable within assignment timeline

**DECISION**: â“ **PENDING USER INPUT**

---

## ğŸ”´ DECISION 3: Connectivity Model

**STATUS**: ğŸ”´ Pending
**PRIORITY**: Critical (core infrastructure)
**DECISION NEEDED BY**: Before network setup

### Option A: Site-to-Site VPN with WireGuard â­ RECOMMENDED

**Implementation**:
```
AWS VPC                               GCP/DO VPC
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     â”‚              â”‚                     â”‚
â”‚  WireGuard Pod      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  WireGuard Pod      â”‚
â”‚  (10.0.1.100:51820) â”‚  UDP 51820  â”‚  (10.1.1.100:51820) â”‚
â”‚        â†“            â”‚              â”‚        â†“            â”‚
â”‚  K8s Services       â”‚              â”‚  K8s Services       â”‚
â”‚  (10.43.0.0/16)     â”‚              â”‚  (10.44.0.0/16)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**WireGuard Configuration**:
```ini
# AWS side (wg0.conf)
[Interface]
PrivateKey = <aws-private-key>
Address = 10.100.0.1/24
ListenPort = 51820

[Peer]
PublicKey = <gcp-public-key>
Endpoint = <gcp-public-ip>:51820
AllowedIPs = 10.100.0.2/32, 10.44.0.0/16
PersistentKeepalive = 25
```

**Pros**:
- âœ… Industry standard VPN protocol
- âœ… **Zero cost** (no VPN gateway fees)
- âœ… Encrypted automatically (ChaCha20)
- âœ… Very fast (kernel-level, minimal overhead)
- âœ… Simple configuration (vs IPsec)
- âœ… Works with any cloud provider
- âœ… Easy to automate with Kubernetes manifests
- âœ… Great for demonstration (show config, test connectivity)

**Cons**:
- âŒ Requires manual key management (but scriptable)
- âŒ Not "managed" (but more control)
- âŒ Need to deploy WireGuard pods in both clusters

**Cost**:
- AWS: $0 (runs on existing nodes)
- GCP/DO: $0 (runs on existing nodes)
- **Total**: $0/month

**Setup Complexity**: Medium (4-5 hours)
- Generate WireGuard keys
- Deploy pods in both clusters
- Configure allowed IPs and routing
- Test connectivity

---

### Option B: AWS VPN Gateway + Cloud VPN

**Implementation**:
```
AWS VPN Gateway                       GCP Cloud VPN
($0.05/hour = $36/month)             ($0.05/hour = $36/month)
        â†“                                     â†“
    AWS VPC                               GCP VPC
```

**Pros**:
- âœ… Managed service (AWS + GCP handle it)
- âœ… Built-in monitoring
- âœ… HA options available
- âœ… Enterprise-grade

**Cons**:
- âŒ **Expensive**: $72/month ($36 AWS + $36 GCP)
- âŒ Complex setup (IPsec config, BGP)
- âŒ Overkill for assignment
- âŒ Not supported by all cloud providers (e.g., DigitalOcean)

**Cost**:
- AWS VPN Gateway: $36/month
- GCP Cloud VPN: $36/month (if GCP) or N/A (if DigitalOcean)
- **Total**: $72/month (if GCP), N/A (if DO)

---

### Option C: Service Mesh (Istio Multi-Cluster)

**Implementation**:
```
AWS K8s + Istio                       GCP K8s + Istio
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Istio Control Plane â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Istio Control Plane â”‚
â”‚        â†“            â”‚  mTLS        â”‚        â†“            â”‚
â”‚ Istio Sidecar Proxy â”‚              â”‚ Istio Sidecar Proxy â”‚
â”‚        â†“            â”‚              â”‚        â†“            â”‚
â”‚  Application Pods   â”‚              â”‚  Application Pods   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros**:
- âœ… Very impressive technically
- âœ… Built-in observability (tracing, metrics)
- âœ… Automatic mTLS (service-to-service encryption)
- âœ… Cross-cluster service discovery
- âœ… Traffic management (retries, circuit breakers)

**Cons**:
- âŒ **Very complex** setup (steep learning curve)
- âŒ Resource overhead (2-3 GB RAM per cluster for Istio)
- âŒ May not fit in free tiers (need larger nodes)
- âŒ Long setup time (8-12 hours for multi-cluster)
- âŒ Overkill for assignment (may not get extra credit)

**Cost**:
- No direct cost, but requires larger nodes
- Estimated: +$20-30/month per cloud (larger instances)

---

### Option D: Bastion + SSH Tunnels

**Implementation**:
```bash
# On local machine
ssh -L 9092:kafka-0.ca3-app:9092 ubuntu@aws-bastion
ssh -L 27017:mongodb-0.ca3-app:27017 ubuntu@gcp-bastion

# Applications connect to localhost:9092, localhost:27017
```

**Pros**:
- âœ… **Cheapest** ($0)
- âœ… Simple to understand
- âœ… Works with any cloud
- âœ… No VPN setup needed

**Cons**:
- âŒ Not production-grade (manual tunnel management)
- âŒ Tunnels can drop (need monitoring/restart)
- âŒ Doesn't work for pod-to-pod (only local machine to pod)
- âŒ Not suitable for automated systems
- âŒ Doesn't demonstrate cloud networking skills

---

### Decision Matrix

| Factor | WireGuard VPN | AWS/GCP VPN | Istio Mesh | SSH Tunnels |
|--------|---------------|-------------|------------|-------------|
| **Cost** | $0 | $72/mo | $20-30/mo | $0 |
| **Complexity** | Medium | High | Very High | Low |
| **Setup Time** | 4-5 hours | 6-8 hours | 12-16 hours | 1 hour |
| **Production-Grade** | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­ |
| **Learning Value** | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­ |
| **Assignment Fit** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­ |
| **Automatable** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­ |

---

### My Recommendation

**RECOMMENDED**: Option A - WireGuard VPN

**Rationale**:
- **Best cost/value**: $0 vs $72/month for managed VPN
- **Industry standard**: WireGuard is modern, widely used
- **Automatable**: Can deploy via Kubernetes manifests
- **Perfect for demo**: Easy to show, test, and fail/recover
- **Works with any cloud**: DigitalOcean, GCP, Azure, etc.

**DECISION**: â“ **PENDING USER INPUT**

---

## ğŸ”´ DECISION 4: Component Distribution

**STATUS**: ğŸ”´ Pending (depends on Topology choice)
**PRIORITY**: High
**DECISION NEEDED BY**: Before deployment planning

### Recommended Distribution (Based on Multi-Cloud Split)

#### AWS Cluster (Keep Existing CA3)

**Data Layer** (Worker-1, t3.medium, 4GB RAM):
- âœ… Kafka StatefulSet (1-3 replicas) - 1.5GB RAM
- âœ… Zookeeper StatefulSet (1 replica) - 512MB RAM
- âœ… MongoDB StatefulSet (1 replica) - 1GB RAM

**Observability Hub** (Master, t3.medium, 4GB RAM):
- âœ… Prometheus (1GB RAM) - Scrapes both clouds
- âœ… Grafana (256MB RAM) - Unified dashboard
- âœ… Loki (512MB RAM) - Centralized logging
- âœ… Alertmanager, Node Exporter

**Why Keep in AWS**:
- Data services benefit from being co-located (Kafka â†” MongoDB low latency)
- Observability hub centralized for easier management
- Already deployed and working (CA3)

---

#### Cloud2 Cluster (GCP/DO - New for CA4)

**Compute Layer** (2 nodes, e.g., 2x e2-small or DO 2x $12/mo droplets):
- â• Producer Deployment (1-3 replicas via HPA) - 128MB RAM each
- â• Processor Deployment (1-3 replicas via HPA) - 256MB RAM each
- â• Promtail DaemonSet - Ship logs to AWS Loki
- â• Node Exporter - Metrics to AWS Prometheus
- â• WireGuard VPN pod

**Why in Cloud2**:
- Demonstrates cross-cloud connectivity (main goal)
- Stateless components (easy to scale/migrate)
- Lower resource requirements (smaller nodes OK)
- Clear failure scenario (VPN down = processors fail)

---

### Data Flow

```
Cloud2 (GCP/DO)                       AWS (us-east-2)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Producer
  â†“
  â”œâ”€â”€[VPN]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Kafka
                                        â†“
                                      (topic: metals-pricing)
                                        â†“
Processor â—„â”€â”€â”€[VPN]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Kafka
  â†“
  â”œâ”€â”€[VPN]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º MongoDB

Promtail â”€â”€â”€[VPN]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Loki (logs)
Node Exp â”€â”€â”€[VPN]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Prometheus (metrics)
```

---

### Resource Requirements

**AWS (Existing)**:
- Master: t3.medium (4GB) - No change
- Worker-1: t3.medium (4GB) - No change
- Worker-2: t3.small (2GB) - **Can remove** (apps move to Cloud2)

**Cloud2 (New)**:
- Node-1: 2GB RAM (Producer + WireGuard)
- Node-2: 2GB RAM (Processor + Promtail)
- Total: 4GB RAM minimum

**Cost Impact**:
- AWS: Remove worker-2 â†’ Save $15/month â†’ New AWS cost: $60/month
- Cloud2: 2 nodes Ã— $20/month = $40/month (DigitalOcean example)
- **Total CA4 Cost**: $60 (AWS) + $40 (DO) = **$100/month**

---

### Alternative Distribution (if budget concern)

**Keep All Data + Observability in AWS**:
- Keep all 3 AWS nodes (don't change CA3)
- **Cloud2**: Only 1 node (2GB RAM) running Producer + Processor

**Cost**:
- AWS: $75/month (no change)
- Cloud2: $20/month (1 small node)
- **Total**: $95/month

---

### My Recommendation

**Option 1** (Optimized):
- AWS: 2 nodes (master + worker-1) = $60/month
- Cloud2: 2 nodes (producers + processors) = $40/month
- **Total**: $100/month

**Rationale**:
- Clean separation (data in AWS, compute in Cloud2)
- Saves $15/month by removing AWS worker-2
- Clear multi-cloud demo

**DECISION**: â“ **PENDING USER INPUT**

---

## ğŸ”´ DECISION 5: Failure Scenario for Resilience Drill

**STATUS**: ğŸ”´ Pending
**PRIORITY**: Medium
**DECISION NEEDED BY**: Before deployment (design system to test)

### Option A: VPN Tunnel Failure â­ RECOMMENDED

**Scenario**:
```bash
# 1. System running normally
kubectl get pods -n ca3-app --context=aws
kubectl get pods -n ca3-app --context=gcp

# 2. Grafana shows healthy metrics across both clouds
open http://localhost:3000

# 3. Kill VPN tunnel
kubectl delete deployment wireguard -n kube-system --context=aws

# 4. Observe failures:
# - Processors (Cloud2) can't reach Kafka (AWS)
# - Producer (Cloud2) can't send messages
# - Grafana shows connection errors, message lag increases
# - Loki stops receiving logs from Cloud2

# 5. Recovery:
kubectl apply -f wireguard.yaml --context=aws

# 6. Verify:
# - VPN re-establishes
# - Processors reconnect
# - Message backlog processes
# - Metrics/logs resume
```

**What This Tests**:
- âœ… Cross-cloud connectivity dependency
- âœ… Graceful degradation (queued messages)
- âœ… Observability during failure (Grafana alerts)
- âœ… Recovery automation
- âœ… Clear cause/effect (VPN down = Cloud2 can't reach AWS)

**Pros**:
- âœ… Tests core CA4 requirement (multi-cloud connectivity)
- âœ… Realistic scenario (network partitions happen)
- âœ… Easy to demonstrate (clear failure, clear recovery)
- âœ… Non-destructive (just restart VPN)

**Cons**:
- âŒ Doesn't test cloud provider failure (more dramatic scenario)

---

### Option B: Entire Cloud Region Failure

**Scenario**:
```bash
# 1. Simulate entire AWS region failure
kubectl delete namespace ca3-app --context=aws

# 2. Observe:
# - All data services down (Kafka, MongoDB)
# - Processors (Cloud2) have nothing to do
# - Grafana down (if in AWS)

# 3. Recovery:
kubectl apply -k k8s/base/ --context=aws

# 4. Wait for:
# - Pods to restart
# - PVCs to reattach
# - Data to be accessible again
```

**What This Tests**:
- âœ… Complete regional failure
- âœ… Recovery from total outage

**Pros**:
- âœ… Dramatic demonstration
- âœ… Tests disaster recovery

**Cons**:
- âŒ Too catastrophic (everything fails)
- âŒ Doesn't highlight multi-cloud benefit
- âŒ Recovery is just "bring AWS back" (not interesting)

---

### Option C: Network Partition (Partial Failure)

**Scenario**:
```bash
# 1. Block specific ports via NetworkPolicy
kubectl apply -f block-kafka-port.yaml --context=aws

# 2. Observe:
# - Processors can ping AWS, but can't reach Kafka port 9092
# - Subtle failure (not obvious)

# 3. Debugging exercise:
# - Check logs
# - Test connectivity
# - Identify blocked port

# 4. Recovery:
kubectl delete networkpolicy block-kafka --context=aws
```

**What This Tests**:
- âœ… Troubleshooting skills
- âœ… Observability (logs, metrics help diagnose)
- âœ… NetworkPolicy understanding

**Pros**:
- âœ… More realistic (subtle failures)
- âœ… Shows troubleshooting process

**Cons**:
- âŒ Less dramatic for video demo
- âŒ Harder to explain in short demo

---

### Option D: Cloud2 Node Failure (Kubernetes Self-Healing)

**Scenario**:
```bash
# 1. Delete processor pod
kubectl delete pod -n ca3-app -l app=processor --context=gcp

# 2. Observe:
# - Kubernetes recreates pod automatically
# - Messages queue in Kafka during downtime
# - HPA may scale up if needed

# 3. Verify:
# - New pod comes up
# - Resumes processing
# - Message lag decreases
```

**What This Tests**:
- âœ… Kubernetes self-healing
- âœ… Pod-level resilience

**Pros**:
- âœ… Shows K8s resilience
- âœ… Fast recovery (30-60 seconds)

**Cons**:
- âŒ Doesn't test multi-cloud aspect (could do in CA3)
- âŒ Already demonstrated in CA3 resilience video

---

### Decision Matrix

| Scenario | Multi-Cloud Focus | Realism | Demo Value | Setup Complexity |
|----------|------------------|---------|-----------|-----------------|
| **VPN Failure** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | Low |
| **Region Failure** | â­â­ | â­â­â­â­â­ | â­â­â­ | Low |
| **Network Partition** | â­â­â­â­ | â­â­â­â­â­ | â­â­â­ | Medium |
| **Pod Failure** | â­â­ | â­â­â­â­ | â­â­â­ | Low |

---

### My Recommendation

**RECOMMENDED**: Option A - VPN Tunnel Failure

**Rationale**:
- Best demonstrates CA4 multi-cloud connectivity
- Clear cause and effect (easy to explain)
- Realistic scenario (network failures are common)
- Shows observability working (Grafana catches it)
- Fast recovery (good for video demo)

**DECISION**: â“ **PENDING USER INPUT**

---

## ğŸ“Š Cost Summary (All Options)

### Scenario 1: GCP + WireGuard VPN + Multi-Cloud Split

| Component | Cost/Month | Duration | Total Cost |
|-----------|-----------|----------|-----------|
| AWS (2 nodes: master + worker-1) | $60 | 1 month | $60 |
| GCP (2 nodes for compute) | $0 (credits) | 3 months | $0 |
| WireGuard VPN | $0 | - | $0 |
| **TOTAL (Free Tier)** | **$60/month** | **3 months** | **$180** |
| **TOTAL (After Credits)** | **$120/month** | - | - |

---

### Scenario 2: DigitalOcean + WireGuard VPN + Multi-Cloud Split

| Component | Cost/Month | Duration | Total Cost |
|-----------|-----------|----------|-----------|
| AWS (2 nodes: master + worker-1) | $60 | 1 month | $60 |
| DigitalOcean (2 nodes for compute) | $0 (credits) | 2 months | $0 |
| WireGuard VPN | $0 | - | $0 |
| **TOTAL (Free Tier)** | **$60/month** | **2 months** | **$120** |
| **TOTAL (After Credits)** | **$100/month** | - | - |

---

### Budget-Friendly Option: DO + 1 Node

| Component | Cost/Month | Duration | Total Cost |
|-----------|-----------|----------|-----------|
| AWS (keep all 3 nodes from CA3) | $75 | 1 month | $75 |
| DigitalOcean (1 small node) | $0 (credits) | 2 months | $0 |
| WireGuard VPN | $0 | - | $0 |
| **TOTAL (Free Tier)** | **$75/month** | **2 months** | **$150** |

---

## ğŸ¯ FINAL RECOMMENDATION SUMMARY

If I were doing this assignment, here's what I'd choose:

### Cloud Provider
**DigitalOcean** (Option C)
- Lowest cost ($100/month after credits vs $120-135 for others)
- Simplest setup (less time debugging)
- Still demonstrates multi-cloud patterns
- 2 months free tier (vs 1 month for Azure)

### Topology
**Multi-Cloud Split** (Option A)
- Data tier in AWS (Kafka, MongoDB, Zookeeper)
- Compute tier in DigitalOcean (Producer, Processor)
- Observability in AWS (Prometheus, Grafana, Loki)

### Connectivity
**WireGuard VPN** (Option A)
- $0 cost (vs $72/month for managed VPN)
- Industry standard
- Automatable with Kubernetes manifests

### Component Distribution
**Optimized**:
- AWS: 2 nodes (master + worker-1) = $60/month
- DO: 2 nodes (compute tier) = $40/month (after credits)

### Failure Scenario
**VPN Tunnel Failure** (Option A)
- Best demonstrates multi-cloud aspect
- Clear, testable, recoverable

### Total Cost
- **During free tier**: $60/month (1 month AWS only, then AWS + DO for 1 month on credits)
- **Assignment duration** (1 month): ~$60
- **After credits**: $100/month

---

## âœ… NEXT STEPS (Once Decisions Made)

Once you approve the decisions above, I'll help you:

1. **Terraform for Cloud2** (DigitalOcean/GCP)
   - Create `terraform/cloud2/` directory
   - Configure Kubernetes cluster
   - Set up networking

2. **WireGuard VPN Setup**
   - Generate keys
   - Deploy WireGuard pods in both clusters
   - Configure routing

3. **Deploy Applications to Cloud2**
   - Migrate Producer/Processor manifests
   - Configure to connect to AWS Kafka/MongoDB via VPN
   - Set up Promtail â†’ AWS Loki

4. **Update Observability**
   - Configure Prometheus to scrape Cloud2
   - Update Grafana dashboards for multi-cloud view
   - Test cross-cloud logging

5. **Resilience Testing**
   - Create failure scenario script
   - Document recovery procedure
   - Record demo video

6. **Documentation**
   - Architecture diagram (multi-cloud)
   - Deployment guide
   - Cost analysis
   - Lessons learned

---

## ğŸ“ DECISION TRACKING

| Decision | Status | Chosen Option | Date | Rationale |
|----------|--------|---------------|------|-----------|
| Cloud Provider | ğŸ”´ Pending | - | - | - |
| Topology | ğŸ”´ Pending | - | - | - |
| Connectivity | ğŸ”´ Pending | - | - | - |
| Component Distribution | ğŸ”´ Pending | - | - | - |
| Failure Scenario | ğŸ”´ Pending | - | - | - |

---

**Last Updated**: November 13, 2025
**Status**: Awaiting user input on 5 critical decisions
